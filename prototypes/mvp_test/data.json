{
  "title": "期末项目研究报告（中文测试）",
  "student_name": "张三",
  "student_id": "20230501",
  "course": "人工智能导论",
  "semester": "2025-2026 第一学期",
  "content_blocks": [
    {
      "type": "text",
      "content": "随着深度学习技术的快速发展，自然语言处理（NLP）领域取得了突破性进展。本项目旨在探究Transformer架构在文本分类任务中的表现。我们希望通过对比实验，验证其相对于传统RNN模型的优势。"
    },
    {
      "type": "section",
      "title": "1. 实验方法"
    },
    {
      "type": "text",
      "content": "我们收集了IMDB电影评论数据集，包含50,000条评论。数据预处理包括去除停用词、分词和向量化。模型训练参数设置为：Learning Rate = 0.001, Batch Size = 32, Epochs = 10。"
    },
    {
      "type": "section",
      "title": "2. 实验结果"
    },
    {
      "type": "text",
      "content": "经过10轮训练，不同模型的准确率对比如下表所示。可以看出，BERT模型取得了最好的效果，达到了93.5%的准确率，显著优于LSTM。"
    },
    {
      "type": "table",
      "headers": ["模型名称", "参数量 (M)", "训练时间 (h)", "准确率 (%)"],
      "rows": [
        ["LSTM (Baseline)", "12.5", "1.5", "87.2"],
        ["Bi-LSTM", "24.0", "2.1", "88.9"],
        ["BERT-Base", "110.0", "5.4", "93.5"]
      ]
    },
    {
      "type": "section",
      "title": "3. 结论"
    },
    {
      "type": "text",
      "content": "本实验成功验证了预训练模型在小样本数据集上的强大泛化能力。未来的工作将集中在模型轻量化研究上，以适应移动端部署的需求。"
    }
  ]
}
